# -*- coding: utf-8 -*-
"""Yantra_FInal_VA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vi16SJ43REXrH7Ng3V54Fi9aom_wUKvx

## **Example use case for the chat bot**
"""

# Install necessary libraries
!pip install rank_bm25 nltk transformers requests

# Import libraries
import os
import nltk
import requests
from nltk.tokenize import word_tokenize
from rank_bm25 import BM25Okapi

# Download necessary NLTK tokenizer
nltk.download('punkt')
nltk.download('punkt_tab')

# Set up Hugging Face API key
HF_API_KEY = "HF_TOKEN"
HF_MODEL = "mistralai/Mistral-7B-Instruct-v0.3"

# Sample documents (Replace this with real data)
documents = [
        "Junction 2 is experiencing heavy congestion due to office rush hour. Vehicle count: 1,500. No accidents. Road is dry but bottlenecks at the roundabout. Signal cycle extended to clear backlog. Congestion expected till 9:00 AM.",

    "Junction 2 has a sudden increase in vehicles due to a metro breakdown. Vehicle count: 2,200. One minor accident near the signal. Road is clear, but pedestrian overflow is slowing traffic. Traffic police deployed, metro authorities alerted. High congestion for the next 45 minutes.",

    "Junction 2 has activated dynamic reallocation due to school zone traffic. Vehicle count: 1,800. No accidents. Road is smooth, but traffic is slow-moving. An additional lane opened temporarily for school drop-offs. Traffic expected to ease by 9:30 AM.",

    "Junction 2 is facing lane blockages due to delivery trucks. Vehicle count: 1,200. No accidents. A roadblock on the service lane is affecting the main road. Delivery trucks diverted via an alternate route. Normal traffic flow expected in 30 minutes.",

    "Junction 2 has stabilized traffic flow with moderate congestion. Vehicle count: 1,000. No accidents. The road is clear but slow-moving due to ongoing metro construction. No major interventions required. Traffic is expected to stay steady till lunch hours.",

    "Junction 2 has light congestion due to lunch-hour vehicle movement. Vehicle count: 900. No accidents. Road is normal, but there is high pedestrian movement. No intervention required. Traffic is expected to be smooth post-2:00 PM.",

    "Junction 2 is experiencing a sudden traffic increase due to a local protest. Vehicle count: 1,700. One minor road rage incident. The road is partially blocked due to a crowd gathering. Traffic diverted, and police are intervening. Heavy congestion expected till 5:00 PM.",

    "Junction 2 has peak office-hour traffic causing slow movement. Vehicle count: 2,500. One bike skidded due to sudden braking. Road is dry, but lane switching is causing slowdowns. Traffic personnel deployed for better management. High congestion expected till 7:30 PM.",

    "Junction 2 traffic is easing, but some congestion remains. Vehicle count: 1,400. No accidents. Normal flow is resuming. No intervention required. Smooth traffic expected post-9:00 PM.",

    "Junction 2 is experiencing night-time truck movement slowing regular traffic. Vehicle count: 1,100. No accidents. The road is clear but slower due to heavy vehicles. Trucks rerouted via a bypass road. Normal flow expected by 11:00 PM.",
]

# Tokenize the documents for BM25
tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]
bm25 = BM25Okapi(tokenized_docs)

# Define the prompt template
prompt_template = """


    The extracted content is:
    {context}



"""

def retrieve_context(query):
    """Retrieve relevant text using BM25."""
    query_tokens = word_tokenize(query.lower())
    scores = bm25.get_scores(query_tokens)
    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:3]
    retrieved_text = "\n".join([documents[i] for i in top_indices])
    return retrieved_text if retrieved_text else "Not applicable"

def query_huggingface(prompt):
    """Query Hugging Face model for response."""
    headers = {"Authorization": f"Bearer {HF_API_KEY}"}
    payload = {
        "inputs": prompt,
        "parameters": {"max_length": 128, "temperature": 0.7},
    }
    response = requests.post(
        f"https://api-inference.huggingface.co/models/{HF_MODEL}",
        headers=headers,
        json=payload
    )
    return response.json()[0]['generated_text'] if response.ok else "Error: Model request failed."

def ask_question(question):
    """Main function to process user question and generate an answer."""
    context = retrieve_context(question)
    final_prompt = prompt_template.format(context=context)
    response = query_huggingface(final_prompt)
    return response

# Example questions
questions = [
    "whats the status of the junction",
]

# Get answers
for q in questions:
    print(f"ðŸ’¬ Answer: {ask_question(q)}\n")

"""General Template"""

# Install necessary libraries
!pip install rank_bm25 nltk transformers requests

# Import libraries
import os
import nltk
import requests
from nltk.tokenize import word_tokenize
from rank_bm25 import BM25Okapi

# Download necessary NLTK tokenizer
nltk.download('punkt')

# Set up Hugging Face API key
HF_API_KEY = "API_TOKEN_HERE"
HF_MODEL = "mistralai/Mistral-7B-Instruct-v0.3"

# Sample documents (Replace this with real data)
documents = [
        "RAW_DATA1_Here.",
        "RAW_DATA2_Here.",
        "RAW_DATA3_Here."
        #ETC...
]

# Tokenize the documents for BM25
tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]
bm25 = BM25Okapi(tokenized_docs)

# Define the prompt template - USAGE OF LESS TOKENS AND PREVENTING HALLUCINATION
prompt_template = """


    The extracted content is:
    {context}


"""

def retrieve_context(query):
    """Retrieve relevant text using BM25."""
    query_tokens = word_tokenize(query.lower())
    scores = bm25.get_scores(query_tokens)
    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:3]
    retrieved_text = "\n".join([documents[i] for i in top_indices])
    return retrieved_text if retrieved_text else "Not applicable"

def query_huggingface(prompt):
    """Query Hugging Face model for response."""
    headers = {"Authorization": f"Bearer {HF_API_KEY}"}
    payload = {
        "inputs": prompt,
        "parameters": {"max_length": 128, "temperature": 0.7},
    }
    response = requests.post(
        f"https://api-inference.huggingface.co/models/{HF_MODEL}",
        headers=headers,
        json=payload
    )
    return response.json()[0]['generated_text'] if response.ok else "Error: Model request failed."

def ask_question(question):
    """Main function to process user question and generate an answer."""
    context = retrieve_context(question)
    final_prompt = prompt_template.format(context=context)
    response = query_huggingface(final_prompt)
    return response

# Example questions
questions = [
    #example questions
    "Junction 2 status around 5pm?",
    "What is the junction status on Friday?",
    #ask any number of questions
]

# Get answers
for q in questions:
    print(f"ðŸ’¬ Answer: {ask_question(q)}\n")